import time
import random
import re
import os
import cv2
import numpy as np
import requests
import urllib.parse
from datetime import datetime
from bs4 import BeautifulSoup
import pandas as pd
import gspread
from oauth2client.service_account import ServiceAccountCredentials

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service


class FixedJobScraper:
    def __init__(self, jobvision_user=None, jobvision_pass=None, headless=False):
        self.jobvision_user = jobvision_user
        self.jobvision_pass = jobvision_pass
        self.jobs = []
        self.setup_browser(headless)

    # ğŸ§­ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ù…Ø±ÙˆØ±Ú¯Ø±
    def setup_browser(self, headless):
        chrome_options = Options()
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        chrome_options.add_experimental_option('prefs', {'profile.default_content_setting_values.notifications': 2})
        if headless:
            chrome_options.add_argument('--headless=new')

        service = Service(executable_path='chromedriver.exe')
        self.driver = webdriver.Chrome(service=service, options=chrome_options)
        self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

    # âœ… Ù…Ø±Ø­Ù„Ù‡ Ù„Ø§Ú¯ÛŒÙ† Ø¨Ù‡ JobVision (Ø¨Ø§ Ú©Ù¾Ú†Ø§)
    def login_to_jobvision(self):
        print("\n========================")
        print("LOGIN TO JOBVISION")
        print("========================")

        self.driver.get("https://account.jobvision.ir/Candidate")
        WebDriverWait(self.driver, 20).until(EC.presence_of_element_located((By.NAME, "Username")))

        # Ø§ÛŒÙ…ÛŒÙ„
        email_input = self.driver.find_element(By.NAME, "Username")
        email_input.send_keys(self.jobvision_user)
        email_input.send_keys(Keys.ESCAPE)
        time.sleep(0.5)

        continue_btn = self.driver.find_element(By.CSS_SELECTOR, "a.btn.btn-primary")
        self.driver.execute_script("arguments[0].click();", continue_btn)
        time.sleep(2)

        # Ø­Ù„ Ú©Ù¾Ú†Ø§
        self.solve_arcaptcha()

        # Ù¾Ø³ÙˆØ±Ø¯
        password_input = WebDriverWait(self.driver, 15).until(
            EC.presence_of_element_located((By.NAME, "Password"))
        )
        password_input.send_keys(self.jobvision_pass)
        time.sleep(0.5)

        login_button = self.driver.find_element(By.CSS_SELECTOR, "button.btn.btn-primary")
        self.driver.execute_script("arguments[0].click();", login_button)
        print("ğŸ” Ø±Ù…Ø² ÙˆØ§Ø±Ø¯ Ø´Ø¯ Ùˆ Ø±ÙˆÛŒ ÙˆØ±ÙˆØ¯ Ú©Ù„ÛŒÚ© Ø´Ø¯.")

        # Ù…Ù†ØªØ¸Ø± Ø§Ù†ØªÙ‚Ø§Ù„ Ø¨Ù‡ ØµÙØ­Ù‡ Ø§ØµÙ„ÛŒ
        WebDriverWait(self.driver, 15).until(EC.url_contains("jobvision.ir"))
        print("ğŸ‰ ÙˆØ±ÙˆØ¯ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯ âœ…")

    # ğŸ§  Ø­Ù„ Ú©Ù¾Ú†Ø§ÛŒ ARCaptcha
    def solve_arcaptcha(self):
        print("ğŸ§  Ø¯Ø± Ø­Ø§Ù„ Ø­Ù„ Ú©Ù¾Ú†Ø§...")
        WebDriverWait(self.driver, 20).until(EC.presence_of_element_located((By.ID, "challenge")))
        time.sleep(1)

        bg_url = self.driver.find_element(By.CSS_SELECTOR, "#challenge .tw-relative img").get_attribute("src")
        piece_url = self.driver.find_element(By.CSS_SELECTOR, "#challenge .tw-absolute img.puzzle").get_attribute("src")

        bg = self.download_image(bg_url)
        piece = self.download_image(piece_url)

        target_x = self.find_gap_position(bg, piece)
        print(f"ğŸ“Œ Ù…ÙˆÙ‚Ø¹ÛŒØª Ù‚Ø·Ø¹Ù‡ Ù¾Ø§Ø²Ù„: {target_x}")

        slider = self.driver.find_element(By.CSS_SELECTOR, "#challenge .draggable")
        actions = ActionChains(self.driver)
        actions.click_and_hold(slider).perform()
        time.sleep(0.2)
        actions.move_by_offset(target_x, 0).perform()
        time.sleep(0.2)
        actions.release().perform()
        time.sleep(2)

    def download_image(self, url):
        r = requests.get(url)
        arr = np.asarray(bytearray(r.content), dtype=np.uint8)
        return cv2.imdecode(arr, cv2.IMREAD_COLOR)

    def find_gap_position(self, bg, piece):
        bg_gray = cv2.cvtColor(bg, cv2.COLOR_BGR2GRAY)
        piece_gray = cv2.cvtColor(piece, cv2.COLOR_BGR2GRAY)
        bg_gray = cv2.GaussianBlur(bg_gray, (3, 3), 0)
        piece_gray = cv2.GaussianBlur(piece_gray, (3, 3), 0)
        bg_edges = cv2.Canny(bg_gray, 100, 200)
        piece_edges = cv2.Canny(piece_gray, 100, 200)
        res = cv2.matchTemplate(bg_edges, piece_edges, cv2.TM_CCOEFF_NORMED)
        _, max_val, _, max_loc = cv2.minMaxLoc(res)

        # Ø¨Ø±Ø§ÛŒ Ø¯ÛŒØ¨Ø§Ú¯
        h, w = piece_edges.shape
        debug = bg.copy()
        cv2.rectangle(debug, max_loc, (max_loc[0]+w, max_loc[1]+h), (0,0,255), 2)
        cv2.imwrite("captcha_debug_match.png", debug)
        return max_loc[0]

    # ğŸ“ Ø§Ø³Ú©Ø±Ù¾ÛŒÙ†Ú¯ JobVision Ø¨Ø¹Ø¯ Ø§Ø² Ù„Ø§Ú¯ÛŒÙ†
    def scrape_jobvision(self, keywords=['Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ', 'machine learning']):
        print("\n========================")
        print("SCRAPING JOBVISION")
        print("========================")

        for keyword in keywords:
            encoded = urllib.parse.quote(keyword)
            url = f"https://jobvision.ir/jobs/keyword/{encoded}"
            self.driver.get(url)
            time.sleep(3)

            for _ in range(3):
                self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(2)

            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            job_links = soup.find_all('a', href=lambda x: x and '/jobs/' in x)

            for link in job_links:
                href = link['href']
                if not href.startswith('http'):
                    href = "https://jobvision.ir" + href

                title = link.get_text(strip=True)
                if not title:
                    continue

                self.jobs.append({
                    'date_added': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    'title': title,
                    'company': 'N/A',
                    'location': 'N/A',
                    'requirements': 'N/A',
                    'salary': 'N/A',
                    'contract_type': 'N/A',
                    'working_hours': 'N/A',
                    'link': href,
                    'source': 'JobVision'
                })
            print(f"âœ… {len(self.jobs)} job(s) found so far.")

    # ğŸ’¾ Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± CSV
    def save_to_csv_append(self, filename='jobs.csv'):
        if not self.jobs:
            return
        df_new = pd.DataFrame(self.jobs)
        if os.path.exists(filename):
            df_old = pd.read_csv(filename, encoding='utf-8-sig')
            combined = pd.concat([df_old, df_new], ignore_index=True).drop_duplicates('link')
            combined.to_csv(filename, index=False, encoding='utf-8-sig')
        else:
            df_new.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"ğŸ’¾ {len(self.jobs)} Ø´ØºÙ„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯ Ø¯Ø± {filename}")

    def close(self):
        self.driver.quit()


# ğŸ Ø§Ø¬Ø±Ø§ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡
if __name__ == "__main__":
    USER = "Ø§ÛŒÙ…ÛŒÙ„_ØªÙˆ"
    PASS = "Ø±Ù…Ø²_ØªÙˆ"

    scraper = FixedJobScraper(jobvision_user=USER, jobvision_pass=PASS)
    scraper.login_to_jobvision()
    scraper.scrape_jobvision(keywords=["Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ", "machine learning"])
    scraper.save_to_csv_append("jobs.csv")
    scraper.close()
