import pandas as pd
import re
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.cluster import KMeans

# -----------------------------
# 1. Ø§Ø³ØªØ§Ù¾â€ŒÙˆØ±Ø¯Ù‡Ø§ÛŒ Ú¯Ø³ØªØ±Ø¯Ù‡ (ÙÙ‚Ø· Ú©Ù„Ù…Ø§Øª ØªÙˆØ¶ÛŒØ­ÛŒ/Ø¹Ù…ÙˆÙ…ÛŒ)
# -----------------------------
stopwords_fa = set([
    # Ú©Ù„Ù…Ø§Øª Ø±Ø¨Ø·ÛŒ Ùˆ Ø­Ø±ÙˆÙ Ø§Ø¶Ø§ÙÙ‡
    "Ù‡Ø§ÛŒ","Ù‡Ø§","Ø¯Ø±","Ø¨Ù‡","Ø¨Ø§","Ø§Ø²","Ø¨Ø±Ø§ÛŒ","ÛŒØ§","Ú©Ù‡","Ù…ÛŒ","Ø±Ø§","Ùˆ","ÛŒÚ©",
    "Ø§ÛŒÙ†","Ø¢Ù†","ØªØ§","Ø¨ÛŒÙ†","Ù‡Ù…","Ø±ÙˆÛŒ","Ú©Ø±Ø¯Ù†","Ø´ÙˆØ¯","Ø§Ø³Øª","Ø¨ÙˆØ¯","Ù†ÛŒØ²","Ø§Ù…Ø§",
    "Ú†ÙˆÙ†","Ø§Ú¯Ø±","Ù‡Ø±","Ù‡Ù…Ù‡","Ø®ÙˆØ¯","Ù†ÛŒØ²","Ø¨Ø§ÛŒØ¯","Ø¯Ø§Ø±Ø§ÛŒ","Ø¯Ø§Ø´ØªÙ†","Ø´Ø¯Ù‡","Ø¨ÙˆØ¯Ù†",
    
    # Ú©Ù„Ù…Ø§Øª Ø¹Ù…ÙˆÙ…ÛŒ Ú©Ù‡ Ù…Ù‡Ø§Ø±Øª Ù†ÛŒØ³ØªÙ†Ø¯
    "Ù‡ÙˆØ´","Ù…ØµÙ†ÙˆØ¹ÛŒ","ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ","Ù…Ø§Ø´ÛŒÙ†","Ø¯Ø§Ø¯Ù‡","Ø³Ø§Ø²ÛŒ","Ù…Ø¯Ù„","Ú©Ø§Ø±","ØªØ¬Ø±Ø¨Ù‡",
    "Ø¢Ø´Ù†Ø§ÛŒÛŒ","Ù…Ù‡Ø§Ø±Øª","ØªÙˆØ³Ø¹Ù‡","ØªÛŒÙ…","ØªØ³Ù„Ø·","Ù¾Ø±ÙˆÚ˜Ù‡","Ù…Ø§Ù†Ù†Ø¯","Ù‡Ù…Ú©Ø§Ø±ÛŒ","Ø§ÛŒ",
    "ØªÙˆØ§Ù†Ø§ÛŒÛŒ","Ø­ÙˆØ²Ù‡","Ù¾Ø±Ø¯Ø§Ø²Ø´","ØªØ­Ù„ÛŒÙ„","Ø·Ø±Ø§Ø­ÛŒ","Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§ÛŒ","Ø¨Ø±","Ø§Ø³ØªÙØ§Ø¯Ù‡",
    "Ù…Ø¯ÛŒØ±ÛŒØª","Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ","Ù¾ÛŒØ´","Ø¨Ù‡ÛŒÙ†Ù‡","Ø³ÛŒØ³ØªÙ…","Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…","Ø¨Ø±Ù†Ø§Ù…Ù‡","Ø­Ù„",
    "Ù…Ø³Ø¦Ù„Ù‡","Ø±ÛŒØ²ÛŒ","Ù†Ø±Ù…","Ø§ÙØ²Ø§Ø±","Ø¹Ù„Ù…ÛŒ","Ù¾ÛŒØ§Ø¯Ù‡","ÙÙ†ÛŒ","Ù‚ÙˆÛŒ","Ø¹Ù…ÛŒÙ‚","Ø³Ø§Ø®ØªÙ†",
    "Ø¯Ø§Ø´ØªÙ†","Ù†ÙˆØ´ØªÙ†","Ø³Ø§Ø®Øª","Ø§Ø¬Ø±Ø§","ÙØ±Ø¢ÛŒÙ†Ø¯","Ø±ÙˆØ´","ØªÚ©Ù†ÛŒÚ©","Ø´ÛŒÙˆÙ‡","Ù†Ø­ÙˆÙ‡",
    "Ø³Ø§Ù„","Ù…Ø§Ù‡","Ø±ÙˆØ²","Ø³Ø§Ø¨Ù‚Ù‡","Ø­Ø¯Ø§Ù‚Ù„","Ø­Ø¯Ø§Ú©Ø«Ø±","Ø¨Ø§Ù„Ø§","Ù¾Ø§ÛŒÛŒÙ†","Ø®ÙˆØ¨","Ø¹Ø§Ù„ÛŒ",
    "Ù‚Ø¯Ø±ØªÙ…Ù†Ø¯","Ù…Ù†Ø§Ø³Ø¨","Ù„Ø§Ø²Ù…","Ø¶Ø±ÙˆØ±ÛŒ","Ù…ÙˆØ±Ø¯","Ù†ÛŒØ§Ø²","Ø´Ø§Ù…Ù„","Ø¯Ø§Ø±Ø¯","Ø¨Ø§Ø´Ø¯",
    "Ø´ÙˆØ¯","Ú¯Ø±Ø¯Ø¯","Ù†Ù…Ø§ÛŒØ¯","Ø¢Ù…ÙˆØ²Ø´","ÛŒØ§Ø¯","Ú¯ÛŒØ±ÛŒ","Ø¯ÛŒØ¯Ù‡","Ø¯Ø§Ù†Ø´","Ù…Ø¹Ø±ÙØª",
    "Ø§Ø·Ù„Ø§Ø¹Ø§Øª","Ù…Ø­ÛŒØ·","Ø´Ø±Ø§ÛŒØ·","Ù…ÙˆÙ‚Ø¹ÛŒØª","ÙØ±ØµØª","Ø²Ù…ÛŒÙ†Ù‡","Ø¨Ø®Ø´","Ù‚Ø³Ù…Øª",
    "Ø¹Ù†ÙˆØ§Ù†","Ù†Ù‚Ø´","ÙˆØ¸ÛŒÙÙ‡","Ù…Ø³Ø¦ÙˆÙ„ÛŒØª","Ø§Ù†Ø¬Ø§Ù…","Ø§Ø±Ø§Ø¦Ù‡","ØªÙˆÙ„ÛŒØ¯","Ø§ÛŒØ¬Ø§Ø¯"
])

stopwords_en = set([
    # Common words
    "the","and","for","with","from","this","that","using","use","you","your",
    "our","are","will","can","should","etc","may","or","of","to","in","on",
    "at","by","as","be","is","was","were","been","being","have","has","had",
    "do","does","did","but","if","then","than","such","so","no","not","only",
    
    # Generic skills/job-related words
    "experience","years","year","skills","knowledge","strong","good","excellent",
    "ability","abilities","work","working","worked","team","project","projects",
    "development","develop","developing","developer","model","models","modeling",
    "data","learning","machine","artificial","intelligence","deep","systems",
    "system","design","designing","production","tools","tool","framework",
    "frameworks","library","libraries","understanding","familiar","familiarity",
    "proficiency","proficient","expert","expertise","background","plus","bonus",
    "required","preferred","nice","must","responsibilities","role","position",
    "job","collaborate","collaboration","build","building","create","creating"
])

# -----------------------------
# 2. Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù…Ù‡Ø§Ø±Øªâ€ŒÙ‡Ø§
# -----------------------------
replace_dict = {
    "Ù¾Ø§ÛŒØªÙˆÙ†": "python",
    "Ø¬Ø§ÙˆØ§": "java",
    "Ø¬Ø§ÙˆØ§Ø§Ø³Ú©Ø±ÛŒÙ¾Øª": "javascript",
    "Ø¬Ø§ÙˆØ§ Ø§Ø³Ú©Ø±ÛŒÙ¾Øª": "javascript",
    "Ù…ØªÙ„Ø¨": "matlab",
    "ØªÙ†Ø³ÙˆØ±ÙÙ„Ùˆ": "tensorflow",
    "Ù¾ÛŒ ØªÙˆØ±Ú†": "pytorch",
    "Ù¾ÛŒâ€ŒØªÙˆØ±Ú†": "pytorch",
    "Ø§Ø³ Ú©ÛŒâ€ŒÙ„Ø±Ù†": "scikit-learn",
    "Ø§Ø³Ú©ÛŒÚ©Øª Ù„Ø±Ù†": "scikit-learn",
    "Ø§Ø³Ú©ÛŒ Ù„Ø±Ù†": "scikit-learn",
    "Ù¾Ø§Ù†Ø¯Ø§Ø²": "pandas",
    "Ù¾Ø§Ù†Ø¯Ø§Ø³": "pandas",
    "Ù†Ø§Ù…Ù¾Ø§ÛŒ": "numpy",
    "Ù†Ø§Ù…Ù¾ÛŒ": "numpy",
    "Ú©Ø±Ø§Ø³": "keras",
    "Ú©Ø±Ø³": "keras",
    "Ø§Ø³Ù¾Ø§Ø±Ú©": "spark",
    "Ø¯Ø§Ú©Ø±": "docker",
    "Ú©ÙˆØ¨Ø±Ù†ØªÛŒØ²": "kubernetes",
    "Ú¯ÛŒØª": "git",
}

def normalize_skills(text):
    if pd.isna(text):
        return ""
    text = text.lower()
    
    # Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†ÛŒ Ù†Ø§Ù…â€ŒÙ‡Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ
    for fa, en in replace_dict.items():
        text = text.replace(fa, en)
    
    # Ø­Ø°Ù Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ø®Ø§Øµ ÙˆÙ„ÛŒ Ù†Ú¯Ù‡â€ŒØ¯Ø§Ø´ØªÙ† + . - # Ú©Ù‡ Ø¯Ø± Ù†Ø§Ù… ØªÚ©Ù†ÙˆÙ„ÙˆÚ˜ÛŒâ€ŒÙ‡Ø§ Ù‡Ø³Øª
    text = re.sub(r"[^Ø¢-ÛŒa-zA-Z0-9\s\+\.\-#]", " ", text)
    
    # Ø­Ø°Ù ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ
    text = " ".join(text.split())
    
    return text

# -----------------------------
# 3. Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
# -----------------------------
df = pd.read_csv("jobs.csv")
df["requirements_normalized"] = df["requirements"].astype(str).apply(normalize_skills)

# -----------------------------
# 4. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù„Ù…Ø§Øª Ù¾Ø±ØªÚ©Ø±Ø§Ø± (Ø¨Ø§ n-grams)
# -----------------------------
vectorizer = CountVectorizer(
    max_df=0.75,          # Ø­Ø°Ù Ú©Ù„Ù…Ø§Øª Ú©Ù‡ Ø¯Ø± Ø¨ÛŒØ´ Ø§Ø² 75% Ø§Ø³Ù†Ø§Ø¯ Ù‡Ø³ØªÙ†Ø¯
    min_df=3,             # Ø¨Ø§ÛŒØ¯ Ø­Ø¯Ø§Ù‚Ù„ Ø¯Ø± 3 Ø¢Ú¯Ù‡ÛŒ ØªÚ©Ø±Ø§Ø± Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ø¯
    ngram_range=(1, 3),   # ØªÚ©â€ŒÚ©Ù„Ù…Ù‡ØŒ Ø¯Ùˆâ€ŒÚ©Ù„Ù…Ù‡ØŒ Ø³Ù‡â€ŒÚ©Ù„Ù…Ù‡
    stop_words=list(stopwords_en) + list(stopwords_fa),
    token_pattern=r'(?u)\b[a-zA-Z][a-zA-Z0-9\+\.\-#]*\b'  # Ø§Ù„Ú¯ÙˆÛŒ Ø§Ù†Ø¹Ø·Ø§Ùâ€ŒÙ¾Ø°ÛŒØ±
)

X = vectorizer.fit_transform(df["requirements_normalized"])
words = vectorizer.get_feature_names_out()
word_counts = X.toarray().sum(axis=0)

# Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø±Ø§Ø³Ø§Ø³ ØªÚ©Ø±Ø§Ø±
freqs = sorted(zip(words, word_counts), key=lambda x: x[1], reverse=True)

# ÙÛŒÙ„ØªØ± Ù†Ù‡Ø§ÛŒÛŒ: Ø­Ø°Ù Ú©Ù„Ù…Ø§Øª Ø®ÛŒÙ„ÛŒ Ú©ÙˆØªØ§Ù‡ ÛŒØ§ Ø§Ø¹Ø¯Ø§Ø¯ Ø®Ø§Ù„Øµ
filtered_freqs = []
for word, count in freqs:
    # Ø­Ø°Ù Ú©Ù„Ù…Ø§Øª Ú©ÙˆØªØ§Ù‡â€ŒØªØ± Ø§Ø² 3 Ú©Ø§Ø±Ø§Ú©ØªØ± (Ù…Ú¯Ø± c++ ÛŒØ§ r)
    if len(word) < 3 and word not in ['r', 'c++', 'c#', 'go']:
        continue
    # Ø­Ø°Ù Ø§Ø¹Ø¯Ø§Ø¯ Ø®Ø§Ù„Øµ
    if word.isdigit():
        continue
    # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¨Ù‡ Ù„ÛŒØ³Øª Ù†Ù‡Ø§ÛŒÛŒ
    filtered_freqs.append((word, count))

print("ğŸ” 50 Ù…Ù‡Ø§Ø±Øª Ù¾Ø±ØªÚ©Ø±Ø§Ø±:")
print("=" * 60)
for word, count in filtered_freqs[:50]:
    percent = 100 * count / len(df)
    print(f"{word:30s}: {count:3d} ({percent:5.1f}%)")

# -----------------------------
# 5. Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ KMeans
# -----------------------------
k = 5  # ØªØ¹Ø¯Ø§Ø¯ Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§
km = KMeans(n_clusters=k, random_state=42, n_init=10)
km.fit(X)

df["cluster"] = km.labels_

# -----------------------------
# 6. Ù†Ù…Ø§ÛŒØ´ Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§ + Ø°Ø®ÛŒØ±Ù‡ CSV
# -----------------------------
print("\n\nğŸ—‚ï¸ Ù†ØªØ§ÛŒØ¬ Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ:")
print("=" * 60)

rows = []
for i in range(k):
    mask = (df["cluster"] == i).to_numpy()
    if mask.sum() == 0:
        continue
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ù‡Ø§Ø±Øªâ€ŒÙ‡Ø§ÛŒ Ø¨Ø±ØªØ± Ø§ÛŒÙ† Ø®ÙˆØ´Ù‡
    top_indices = X[mask].sum(axis=0).A1.argsort()[-20:][::-1]
    top_words = [words[j] for j in top_indices]
    
    # ÙÛŒÙ„ØªØ± Ù†Ù‡Ø§ÛŒÛŒ
    filtered_words = [w for w in top_words if len(w) >= 3 or w in ['r', 'c++', 'c#']]
    
    print(f"\n--- Ø®ÙˆØ´Ù‡ {i} ({mask.sum()} Ø¢Ú¯Ù‡ÛŒ) ---")
    print("Ù…Ù‡Ø§Ø±Øªâ€ŒÙ‡Ø§:", ", ".join(filtered_words[:15]))
    
    for w in filtered_words[:15]:
        rows.append({"cluster": i, "skill": w, "frequency": X[mask, vectorizer.vocabulary_[w]].sum()})

# Ø°Ø®ÛŒØ±Ù‡ Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ø§ ÙØ±Ú©Ø§Ù†Ø³
output_df = pd.DataFrame(rows)
output_df = output_df.sort_values(['cluster', 'frequency'], ascending=[True, False])
output_df.to_csv("skills_clusters.csv", index=False, encoding="utf-8-sig")

# Ø°Ø®ÛŒØ±Ù‡ Ø®Ù„Ø§ØµÙ‡ Ø¢Ù…Ø§Ø±ÛŒ
summary_rows = []
for word, count in filtered_freqs[:100]:
    percent = 100 * count / len(df)
    summary_rows.append({"skill": word, "count": count, "percentage": round(percent, 1)})

summary_df = pd.DataFrame(summary_rows)
summary_df.to_csv("skills_summary.csv", index=False, encoding="utf-8-sig")

print("\n\nğŸ’¾ Ù†ØªØ§ÛŒØ¬ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯:")
print(f"   ğŸ“„ skills_clusters.csv - {len(output_df)} Ù…Ù‡Ø§Ø±Øª Ø¯Ø± {k} Ø®ÙˆØ´Ù‡")
print(f"   ğŸ“„ skills_summary.csv - 100 Ù…Ù‡Ø§Ø±Øª Ø¨Ø±ØªØ±")
print(f"\n   ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø¢Ú¯Ù‡ÛŒ: {len(df)}")
print(f"   ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬â€ŒØ´Ø¯Ù‡: {len(words)}")